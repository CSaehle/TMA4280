\section{The Poisson Problem}

Poisson's equation is defined as

\begin{align}
  - \nabla ^2 u &= f &\text{in~} \Omega \\
  u &= 0 &\text{on~} \partial\Omega
\end{align}
where $\nabla^2$ is the Laplacian, the second order differential
operator. {\em u} and {\em f} are both real- or complex-valued functions on a
manifold, in our case the Euclidian space.

We discretize this problem on a uniform, finite, difference grid with a mesh
spacing $h = 1/n$. To obtain the second derivative, one approximate $u$ by using
the 5-point stencil. The problem can thus be expressed as

\begin{equation}
  -\cfrac{u_{i+1, j} + u_{i, j+1} +u_{i-1, j} +u_{i, j-1} - 4u_{i, j}}{h^2} =
  f_{i,j}
\end{equation}

where $f_{i,j} = f(x_i, y_j)$ and $u_{i,j} \approx u(x_i, y_j)$, the
approximation.

Now, our problem is reduced to find a solution to

\begin{equation}
  \underline A \underline u = \underline f
\end{equation}

where $\underline A$ represents the discrete Laplace operator, $\underline u$ is
a vector representing the unknowns, and $\underline f$ is a known right hand
side vector.

Now, let $\underline T = \underline A$, and let $\underline U$ be a matrix with
the same size as $\underline T$. By expressing the partial derivatives of both
$x$ and $y$ with $\underline T$ and $\underline U$, we can solve this problem as
a system of equation. Thus, let

\begin{align}
  \cfrac{1}{h^2}(\underline{TU})i,j \simeq& -(\cfrac{\partial^2u}{\partial x^2})i,j \\
  \cfrac{1}{h^2}(\underline{UT})i,j \simeq& -(\cfrac{\partial^2u}{\partial y^2})i,j
\end{align}

and express Equation 1 through $\underline T$ and $\underline U$ as follows:

\begin{equation}
  (\underline{TU} + \underline{UT})i,j = h^2f_{i,j} = \underline{G}
\end{equation}

Henceforth, our goal is now to solve

\begin{equation}
  \underline{TU} + \underline{UT} = \underline G
\end{equation}

where $\underline T$, $\underline U$, $\underline G \in \mathbb{M}_{M\times M}$.

\subsection{Diagonalization}

We thus have to find the eigenvalues and the orthonormal eigenvectors to
$\underline T$ and place them into $\underline Q$, an orthonormal matrix. As
$\underline Q$ represent all the orthonormal eigenvectors, we have that

\begin{equation}
  \underline T = \underline Q \underline \Lambda \underline Q^T
\end{equation}
where $\underline \Lambda$ is the diagonal matrix with all the eigenvalues to
$\underline T$.

By combining Equation 8 and 9, we get
\begin{align}
  \underline {Q \Lambda Q^T U} + \underline {U Q \Lambda Q^T} &= \underline G \\
  \underline {\Lambda Q^T U Q} + \underline {Q^T U Q \Lambda} &= \underline{Q^T
    G Q} \\
  \underline {\Lambda \tilde{U}} + \underline {\tilde{U} \Lambda} &= \underline{\tilde{G}}
\end{align}

where
\begin{equation}
  \underline U = \underline{Q^T \tilde{U} Q}
\end{equation}

Out from this, we can now start on different method solving this issue.

\subsection{Methods}

The first we need to do, is to compute $\underline{\tilde{G}} = \underline Q^T
(h^2 \underline F) \underline Q$, which is essentially two matrix
multiplications as all these variables can be found. This will take
$\mathcal{O}(n^3)$ operations\footnote{Or $n^{\log_2 1 + o(1)}$, when using
  the Strassen algorithm.}.

The next we need to find is $\underline{\tilde{U}}$. From Equation 12, this is
trivial:

\begin{align*}
  \underline {\Lambda \tilde{U}} + \underline {\tilde{U} \Lambda} &=
  \underline{\tilde{G}}\\
  \lambda_i\tilde{u}_{i,j} + \lambda_j\tilde{u}_{i,j} &= \tilde{g}_{i,j} \\
  (\lambda_i + \lambda_j)\tilde{u}_{i,j} &= \tilde{g}_{i,j} \\
  \tilde{u}_{i,j} &= \cfrac{\tilde{g}_{i,j}}{\lambda_i + \lambda_j}
\end{align*}

It then follows that finding $\underline{\tilde{U}}$ can be done in
$\mathcal{O}(n^2)$ time.

Finding the original $\underline U$ is trivial by using Equation 13, which uses
$\mathcal{O}(n^3)$ time. When this calculation is done, we have thus solved the
Poisson problem in $\mathcal{O}(n^3)$ time.

\subsection{Improvements}

The implementation used in this exercise runs in $\mathcal{O}(n^2\log (n))$,
which is faster than the $\mathcal{O}(n^3)$ solution described above. To avoid
repeatability, a good explanation on how this solution works is explained in
great detail in ``{\em The Poisson Problem in $\mathbb{R}^2$: Fast
  Diagonalization Methods}'' (Einar M. RÃ¸nquist, 2011) and the reader is
encouraged to read this paper.

A very brief explanation of this implementation is that we look at how the
matrix multiplication is done. A matrix multiplication $\underline A \times
\underline B$ can be interpreted as a matrix $\underline A$ multiplied with $n$
vectors $\underline b_i$, which has the complexity $\mathcal{O}(n~n^2)$.
However, for each matrix multiplication $\underline A \underline b_i$, we can,
by using some assumptions, use the discrete sine transform to perform the
multiplication. As a discrete sine transform is using $\mathcal{O}(n \log (n))$
time, the total time is thus $\mathcal{O}(n^2\log (n))$ instead of the original
$\mathcal{O}(n^3)$.

